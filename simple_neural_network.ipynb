{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731db958-03a0-4ba0-b9a4-fa242dc507a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 2  # Reducido para dataset pequeño\n",
    "IMAGE_SIZE = (224, 224)  # Tamaño estándar para transfer learning\n",
    "EPOCHS = 50  # Más épocas pero con early stopping\n",
    "DATASET_DIR = \"your_dataset\" #Se encuentran las carpetas que contienen las imagenes para entrenar el modelo. En esta caso positive and negative\n",
    "\n",
    "# Cargar dataset con data augmentation\n",
    "def create_augmented_dataset():\n",
    "    # Data augmentation para amplificar el dataset\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomBrightness(0.2),\n",
    "        layers.RandomContrast(0.2),\n",
    "    ])\n",
    "    \n",
    "    # Dataset original\n",
    "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        DATASET_DIR,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Aplicar augmentation\n",
    "    augmented_dataset = dataset.map(\n",
    "        lambda x, y: (data_augmentation(x, training=True), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Combinar dataset original + augmentado\n",
    "    combined_dataset = dataset.concatenate(augmented_dataset)\n",
    "    combined_dataset = combined_dataset.concatenate(augmented_dataset)  # Triplicar datos\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "class_names = ['negative', 'positive']\n",
    "print(\"Clases:\", class_names)\n",
    "\n",
    "# Crear dataset aumentado\n",
    "dataset = create_augmented_dataset()\n",
    "\n",
    "# Función mejorada para desempaquetar\n",
    "def unpack_and_convert(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in dataset:\n",
    "        batch_images, batch_labels = batch\n",
    "        for i in range(len(batch_images)):\n",
    "            # Normalizar las imágenes a [0,1]\n",
    "            image = tf.cast(batch_images[i], tf.float32) / 255.0\n",
    "            label = tf.cast(batch_labels[i], tf.int32)\n",
    "            \n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Aplicar la función\n",
    "images, labels = unpack_and_convert(dataset)\n",
    "total = len(images)\n",
    "print(f\"Total de ejemplos (con augmentation): {total}\")\n",
    "\n",
    "# Dividir los datos (70/15/15 para dataset pequeño)\n",
    "train_size = int(0.7 * total)\n",
    "val_size = int(0.15 * total)\n",
    "\n",
    "# Mezclar los datos antes de dividir\n",
    "indices = np.random.permutation(total)\n",
    "images = [images[i] for i in indices]\n",
    "labels = [labels[i] for i in indices]\n",
    "\n",
    "train_images = images[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "val_images = images[train_size:train_size + val_size]\n",
    "val_labels = labels[train_size:train_size + val_size]\n",
    "test_images = images[train_size + val_size:]\n",
    "test_labels = labels[train_size + val_size:]\n",
    "\n",
    "print(f\"Entrenamiento: {len(train_images)} ejemplos\")\n",
    "print(f\"Validación: {len(val_images)} ejemplos\")\n",
    "print(f\"Prueba: {len(test_images)} ejemplos\")\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "# Configurar datasets con más augmentation en entrenamiento\n",
    "def augment_train_data(image, label):\n",
    "    # Augmentation adicional solo para entrenamiento\n",
    "    image = tf.image.random_brightness(image, 0.1)\n",
    "    image = tf.image.random_saturation(image, 0.7, 1.3)\n",
    "    image = tf.image.random_hue(image, 0.1)\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.map(augment_train_data, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(200).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Modelo con Transfer Learning (más robusto para pocos datos)\n",
    "def create_transfer_model():\n",
    "    # Cargar modelo preentrenado\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Congelar las primeras capas\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Agregar capas personalizadas\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "model = create_transfer_model()\n",
    "\n",
    "# Compilar con learning rate más bajo\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks para evitar overfitting\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluar en conjunto de prueba\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Precisión en prueba: {test_accuracy:.4f}\")\n",
    "\n",
    "# Función de predicción mejorada\n",
    "def predict_snake_improved(image_path, show_confidence_details=True):\n",
    "    try:\n",
    "        # Cargar y preprocesar la imagen\n",
    "        img = tf.keras.utils.load_img(image_path, target_size=(224, 224))\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        img_array = tf.cast(img_array, tf.float32) / 255.0\n",
    "        img_array = tf.expand_dims(img_array, 0)\n",
    "        \n",
    "        # Hacer predicción\n",
    "        prediction = model.predict(img_array, verbose=0)[0][0]\n",
    "        \n",
    "        if show_confidence_details:\n",
    "            print(f\"Valor raw de predicción: {prediction:.4f}\")\n",
    "            print(f\"Umbral de decisión: 0.5\")\n",
    "        \n",
    "        # Interpretar resultado con umbral ajustable\n",
    "        threshold = 0.5\n",
    "        if prediction > threshold:\n",
    "            confidence = prediction\n",
    "            result = f\"Morelia viridis (confidence: {confidence:.1%})\"\n",
    "        else:\n",
    "            confidence = 1 - prediction\n",
    "            result = f\"NOT Morelia viridis (confidence: {confidence:.1%})\"\n",
    "        \n",
    "        return result, prediction\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", None\n",
    "\n",
    "# Función para evaluar múltiples imágenes y ajustar umbral\n",
    "def evaluate_predictions(image_paths, true_labels):\n",
    "    \"\"\"\n",
    "    Evalúa múltiples imágenes para encontrar el mejor umbral\n",
    "    image_paths: lista de rutas de imágenes\n",
    "    true_labels: lista de etiquetas verdaderas (1 para serpiente, 0 para no serpiente)\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            img = tf.keras.utils.load_img(path, target_size=(224, 224))\n",
    "            img_array = tf.keras.utils.img_to_array(img)\n",
    "            img_array = tf.cast(img_array, tf.float32) / 255.0\n",
    "            img_array = tf.expand_dims(img_array, 0)\n",
    "            pred = model.predict(img_array, verbose=0)[0][0]\n",
    "            predictions.append(pred)\n",
    "        except:\n",
    "            predictions.append(0.5)  # Valor neutro si hay error\n",
    "    \n",
    "    # Probar diferentes umbrales\n",
    "    best_threshold = 0.5\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "        pred_labels = [1 if p > threshold else 0 for p in predictions]\n",
    "        accuracy = sum(p == t for p, t in zip(pred_labels, true_labels)) / len(true_labels)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Mejor umbral encontrado: {best_threshold:.2f}\")\n",
    "    print(f\"Precisión con este umbral: {best_accuracy:.2%}\")\n",
    "    \n",
    "    return best_threshold, predictions\n",
    "\n",
    "# Graficar resultados\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "plt.title('Precisión del modelo')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.title('Pérdida del modelo')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e0a6f-2b1f-42bc-a850-c2aa2d98e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar la función mejorada\n",
    "image_path = \"target_image\"\n",
    "resultado, pred_value = predict_snake_improved(image_path)\n",
    "print(resultado)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c034f6-c53a-4f4c-803c-91c64e732670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
